{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cc76690",
   "metadata": {},
   "source": [
    "# Data Processing: Chunking and Vector Store Creation\n",
    "\n",
    "This notebook executes the data processing pipeline to prepare our complaint data for a retrieval system. It uses a modular script, `build_vector_store.py`, which handles two main tasks:\n",
    "\n",
    "1.  **Chunking**: Breaks down long complaint narratives into smaller, manageable chunks.\n",
    "2.  **Embedding & Indexing**: Converts these chunks into numerical vectors (embeddings) and stores them in a FAISS vector store for efficient similarity search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd46a157",
   "metadata": {},
   "source": [
    "### Step 1: Import Main Functions\n",
    "\n",
    "We import the necessary functions from our modular script and define the key parameters for our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c06a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from build_vector_store import chunk_complaints, create_and_save_faiss_index\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_DATA_DIR = '../data'\n",
    "FILTERED_CSV_PATH = os.path.join(BASE_DATA_DIR, 'filtered_complaints_2.csv')\n",
    "CHUNKED_CSV_PATH = os.path.join(BASE_DATA_DIR, 'chunked_complaints_500_100.csv')\n",
    "VECTOR_STORE_PATH = '../vector_store'\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef5546b",
   "metadata": {},
   "source": [
    "### Step 2: Load Filtered Data\n",
    "\n",
    "Load the cleaned and filtered dataset created during the EDA phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff26f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv(FILTERED_CSV_PATH)\n",
    "    print(f\"✅ Loaded filtered complaints: {df.shape}\")\n",
    "    display(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: File not found at {FILTERED_CSV_PATH}. Please run the EDA notebook first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d6ef21",
   "metadata": {},
   "source": [
    "### Step 3: Chunk Complaint Narratives\n",
    "\n",
    "We'll now chunk the `cleaned_narrative` column to prepare it for the embedding model. This process can take a few minutes for a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd7efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if chunked data already exists to save time\n",
    "if not os.path.exists(CHUNKED_CSV_PATH):\n",
    "    chunked_df = chunk_complaints(\n",
    "        df=df,\n",
    "        text_column='cleaned_narrative',\n",
    "        id_column='Complaint ID',\n",
    "        product_column='Product',\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "    # Save the intermediate chunked data\n",
    "    chunked_df.to_csv(CHUNKED_CSV_PATH, index=False)\n",
    "    print(f\"✅ Chunked data saved to: {CHUNKED_CSV_PATH}\")\n",
    "else:\n",
    "    print(f\"Chunked data file already exists at {CHUNKED_CSV_PATH}. Loading it directly.\")\n",
    "    chunked_df = pd.read_csv(CHUNKED_CSV_PATH)\n",
    "\n",
    "print(f\"\\n✅ Loaded chunked complaints dataframe: {chunked_df.shape}\")\n",
    "display(chunked_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0acdd97",
   "metadata": {},
   "source": [
    "### Step 4: Create and Save FAISS Vector Store\n",
    "\n",
    "This final step converts the text chunks into vector embeddings and builds the FAISS index. This is computationally intensive and will take a significant amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the indexing process\n",
    "create_and_save_faiss_index(\n",
    "    chunk_df=chunked_df,\n",
    "    embedding_model_name=EMBEDDING_MODEL,\n",
    "    vector_store_path=VECTOR_STORE_PATH\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
